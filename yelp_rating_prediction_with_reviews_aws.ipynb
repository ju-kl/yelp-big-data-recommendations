{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "#sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "#sc.install_pypi_package(\"matplotlib\")\n",
    "#sc.install_pypi_package(\"spark-nlp==2.5.4\")\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nlp\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "# spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "# spark ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, CountVectorizer, Word2Vec\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "# spark nlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/notebook/work', '/opt/conda/lib/python37.zip', '/opt/conda/lib/python3.7', '/opt/conda/lib/python3.7/lib-dynload', '', '/opt/conda/lib/python3.7/site-packages', '/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/IPython/extensions', '/home/notebook/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/conda\n"
     ]
    }
   ],
   "source": [
    "!which conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): failed\n",
      "\n",
      "CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/johnsnowlabs/linux-64/current_repodata.json>\n",
      "Elapsed: -\n",
      "\n",
      "An HTTP error occurred when trying to retrieve this URL.\n",
      "HTTP errors are often intermittent, and a simple retry will get you on your way.\n",
      "'https://conda.anaconda.org/johnsnowlabs/linux-64'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c johnsnowlabs spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f5aaaa54250>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/spark-nlp/\u001b[0m\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spark-nlp==2.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ed47c9bf6b43a1ada7f08574b2e4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start spark session (from https://nlp.johnsnowlabs.com/docs/en/install#databricks) & script Ashish\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[8]\")\\\n",
    "    .config(\"spark.driver.memory\",\"4g\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Platforms\n",
    "\n",
    "**Goal of this notebook:** Build a system that recommends a rating based on the review written by the user using databricks.\n",
    "\n",
    "**Resources used:**\n",
    "- Installations:\n",
    "  - https://www.youtube.com/watch?v=TNX_GShSyHc\n",
    "  - https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html#Getting%20Started.html\n",
    "\n",
    "- NLP:\n",
    "  - https://towardsdatascience.com/natural-language-processing-in-apache-spark-using-nltk-part-1-2-58c68824f660\n",
    "  - https://towardsdatascience.com/natural-language-processing-with-pyspark-and-spark-nlp-b5b29f8faba\n",
    "  - https://medium.com/analytics-vidhya/nlp-preprocessing-pipeline-what-when-why-2fc808899d1f\n",
    "  - https://www.analyticsvidhya.com/blog/2020/07/build-text-categorization-model-with-spark-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03044074522946baa992d43ed16c2155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define location variables\n",
    "AWS_BUCKET_NAME = \"big-data-class-final-project\"\n",
    "FILE_NAME = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# load data\n",
    "review = spark.read.json(\"s3a://\" + str(AWS_BUCKET_NAME) + \"/\" + str(FILE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3f5b92470e4f2ea500c33b0d878b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430620a695974df1865468f706806c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 8021122 observations.\n",
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|  2.0|As someone who ha...|\n",
      "|  1.0|I am actually hor...|\n",
      "|  5.0|I love Deagan's. ...|\n",
      "|  1.0|Dismal, lukewarm,...|\n",
      "|  4.0|Oh happy day, fin...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# select stars and text for now\n",
    "dat = review.select(\"stars\", \"text\")\n",
    "\n",
    "# print length of data\n",
    "print(\"The dataset has {} observations.\".format(dat.count()))\n",
    "\n",
    "# rename text col to label\n",
    "dat = dat.withColumnRenamed(\"stars\", \"label\")\n",
    "\n",
    "# display\n",
    "dat.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# initialize StopWordsRemover\n",
    "stopword_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "# initialize hashingTF\n",
    "hashingTF = HashingTF(inputCol=stopword_remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# build pipeline\n",
    "pipeline_1 = Pipeline(stages=[tokenizer, stopword_remover, hashingTF])\n",
    "\n",
    "# fit pipeline to data\n",
    "dat_encoded_1 = pipeline_1.fit(dat).transform(dat)\n",
    "\n",
    "# show\n",
    "dat_encoded_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_df, test_df = dat_encoded_1.randomSplit([.8,.2],seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# make prediction\n",
    "lrm = lr.fit(train_df)\n",
    "\n",
    "# create predictions df\n",
    "predictions = lrm.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# evaluate predictions\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
