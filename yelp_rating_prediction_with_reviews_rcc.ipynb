{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "# spark nlp\n",
    "#from sparknlp.annotator import LemmatizerModel\n",
    "\n",
    "# spark ML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Platforms\n",
    "\n",
    "**Goal of this notebook:** Build a system that recommends a rating based on the review written by the user using databricks.\n",
    "\n",
    "**Resources used:**\n",
    "- Installations:\n",
    "  - https://www.youtube.com/watch?v=TNX_GShSyHc\n",
    "  - https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html#Getting%20Started.html\n",
    "\n",
    "- NLP:\n",
    "  - https://towardsdatascience.com/natural-language-processing-in-apache-spark-using-nltk-part-1-2-58c68824f660\n",
    "  - https://towardsdatascience.com/natural-language-processing-with-pyspark-and-spark-nlp-b5b29f8faba\n",
    "  - https://medium.com/analytics-vidhya/nlp-preprocessing-pipeline-what-when-why-2fc808899d1f\n",
    "  - https://www.analyticsvidhya.com/blog/2020/07/build-text-categorization-model-with-spark-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read necessary data into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark session\n",
    "spark = SparkSession.builder.appName('rating_prediction_with_reviews').getOrCreate()\n",
    "\n",
    "# change configuration settings on Spark \n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), \n",
    "                                        ('spark.app.name', 'Spark Updated Conf'), \n",
    "                                        ('spark.executor.cores', '4'), \n",
    "                                        ('spark.cores.max', '4'), \n",
    "                                        ('spark.driver.memory','8g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location variables\n",
    "FILE_PATH = \"hdfs://nameservice1/user/vvenkatesan/final_project/\"\n",
    "#FILE_NAME = \"yelp_academic_dataset_review.json\"\n",
    "FILE_NAME = \"review_subset.json\"\n",
    "\n",
    "# load data from hdfs\n",
    "review = spark.read.json(str(FILE_PATH) + str(FILE_NAME))\n",
    "\n",
    "# load data from hive\n",
    "#review = sqlContext.sql(\"select * from big_data_group_2.review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# investigate data types\n",
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important features are really the stars (target variable) and text (features) for the prediction of rating with reviews. The remaining features might be stripped down in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 400430 observations.\n",
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|  1.0|\"Beware  of the m...|\n",
      "|  1.0|$99 dollar specia...|\n",
      "|  1.0|(This is for the ...|\n",
      "|  1.0|***************bu...|\n",
      "|  1.0|***DO NOT TAKE YO...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prep files depending on which file is loaded (subset vs. full file)\n",
    "if FILE_NAME == \"review_subset.json\":\n",
    "    # no prep necessary\n",
    "    dat = review\n",
    "else:\n",
    "    # select stars and text for now\n",
    "    dat = review.select(\"stars\", \"text\")\n",
    "    # rename text col to label\n",
    "    dat = dat.withColumnRenamed(\"stars\",\"label\")\n",
    "\n",
    "# print length of data\n",
    "print(\"The dataset has {} observations.\".format(dat.count()))\n",
    "    \n",
    "# display\n",
    "dat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_df, test_df = dat.randomSplit([.8,.2],seed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation 1 : HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# initialize StopWordsRemover\n",
    "stopword_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "# initialize hashingTF\n",
    "hashingTF = HashingTF(inputCol=stopword_remover.getOutputCol(), outputCol=\"raw_features\", numFeatures=1000)\n",
    "\n",
    "# initialize idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# build pipeline\n",
    "pipeline_1 = Pipeline(stages=[tokenizer, stopword_remover, hashingTF, idf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation 2 : CountVecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# initialize StopWordsRemover\n",
    "stopword_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "\n",
    "# initialize countVectorizer\n",
    "countVectorizer = CountVectorizer(inputCol=stopword_remover.getOutputCol(), outputCol=\"features\", vocabSize=1000)\n",
    "\n",
    "# build pipeline\n",
    "pipeline_2 = Pipeline(stages=[tokenizer, stopword_remover, countVectorizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3147435425769072\n",
      "This program took 1.04 minutes to run.\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# extend pipeline\n",
    "pipeline_model_1 = Pipeline(stages=[pipeline_1, lr])\n",
    "\n",
    "# make prediction\n",
    "lrm_1 = pipeline_model_1.fit(train_df)\n",
    "\n",
    "# create predictions df\n",
    "predictions = lrm_1.transform(test_df)\n",
    "\n",
    "# evaluate predictions\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
    "\n",
    "# stop timer\n",
    "t1 = time.time()\n",
    "\n",
    "# insight runtime\n",
    "print(\"This program took {:.2f} minutes to run.\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On subset:\n",
    "1.325792785296943\n",
    "This program took 0.45 minutes to run.\n",
    "- On full data:\n",
    "1.3147435425769072\n",
    "This program took 1.04 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0948662025083467\n",
      "This program took 0.93 minutes to run.\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# extend pipeline\n",
    "pipeline_model_2 = Pipeline(stages=[pipeline_2, lr])\n",
    "\n",
    "# make prediction\n",
    "lrm_2 = pipeline_model_2.fit(train_df)\n",
    "\n",
    "# create predictions df\n",
    "predictions = lrm_2.transform(test_df)\n",
    "\n",
    "# evaluate predictions\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
    "\n",
    "# stop timer\n",
    "t1 = time.time()\n",
    "\n",
    "# insight runtime\n",
    "print(\"This program took {:.2f} minutes to run.\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0982139117666376\n",
      "This program took 1.42 minutes to run.\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# extend pipeline\n",
    "pipeline_model_2 = Pipeline(stages=[pipeline_2, lr])\n",
    "\n",
    "# make prediction\n",
    "lrm_2 = pipeline_model_2.fit(train_df)\n",
    "\n",
    "# create predictions df\n",
    "predictions = lrm_2.transform(test_df)\n",
    "\n",
    "# evaluate predictions\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
    "\n",
    "# stop timer\n",
    "t1 = time.time()\n",
    "\n",
    "# insight runtime\n",
    "print(\"This program took {:.2f} minutes to run.\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On subset:\n",
    "0.9081573127054362\n",
    "This program took 1.22 minutes to run.\n",
    "- On full data:\n",
    "1.0982139117666376\n",
    "This program took 1.42 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "- Pipeline 1 (hashingTF and IDF) yields worse results than pipeline 2 (CountVectorizer) in this case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9451034399092813\n",
      "This program took 2.17 minutes to run.\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize random forest model\n",
    "rf = RandomForestClassifier(maxDepth=5, numTrees=15, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# extend pipeline\n",
    "pipeline_model_rf_1 = Pipeline(stages=[pipeline_1, rf])\n",
    "\n",
    "# make prediction\n",
    "rfm_1 = pipeline_model_rf_1.fit(train_df)\n",
    "\n",
    "# create predictions df\n",
    "predictions = rfm_1.transform(test_df)\n",
    "\n",
    "# evaluate predictions\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
    "\n",
    "# stop timer\n",
    "t1 = time.time()\n",
    "\n",
    "# insight runtime\n",
    "print(\"This program took {:.2f} minutes to run.\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On subset:\n",
    "1.934188655062775\n",
    "This program took 0.53 minutes to run.\n",
    "- On full data:\n",
    "1.9451034399092813\n",
    "This program took 2.17 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.328617320571232\n",
      "This program took 2.56 minutes to run.\n"
     ]
    }
   ],
   "source": [
    "# start timer\n",
    "t0 = time.time()\n",
    "\n",
    "# initialize random forest model\n",
    "rf = RandomForestRegressor(maxDepth=5, numTrees=15, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# extend pipeline\n",
    "pipeline_model_rf_1 = Pipeline(stages=[pipeline_1, rf])\n",
    "\n",
    "# make prediction\n",
    "rfm_1 = pipeline_model_rf_1.fit(train_df)\n",
    "\n",
    "# create predictions df\n",
    "predictions = rfm_1.transform(test_df)\n",
    "\n",
    "# evaluate predictions\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"}))\n",
    "\n",
    "# stop timer\n",
    "t1 = time.time()\n",
    "\n",
    "# insight runtime\n",
    "print(\"This program took {:.2f} minutes to run.\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On subset:\n",
    "1.3261032099661794\n",
    "This program took 0.59 minutes to run.\n",
    "- On full data:\n",
    "1.328617320571232\n",
    "This program took 2.56 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "- Using the Random Forest Regressor yields better resutls than using the Random Forest Classifier \n",
    "- Overall, the Random Forest performs worse than the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
