Documentation final project:

rcc notebook:
1. Connect to cisco vpn

2. Launch jupyter: https://hadoop.rcc.uchicago.edu/user/kleindiek/tree

3. Start desired notebook



AWS:
1. Create free tier account on aws

2. Create S3 bucket

3. Upload data into S3 bucket

4. Create user with read access to S3 bucket under IAM
-- choose programatic access type
-- choose attach existing policies directly, search for S3, choose AmazonS3FullAccess
-- skip tags
-- hit create
-- retrieve access key ID and secret access key for user; save .csv

5. Create cluster using aws EMR
-- select S3 bucket created above
-- release label:emr-5.30.1
-- hadoop distribution:Amazon 2.8.5
-- applications: Spark: Spark 2.4.5 on Hadoop 2.8.5 YARN and Zeppelin 0.8.2
-- selected 1 master and 2 core nodes
-- add bootstrap action script (see: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub-install-kernels-libs.html)

6. Start notebook on EMR cluster: https://e-cxpszt7i8v76s2jssvthopbe.emrnotebooks-prod.us-east-1.amazonaws.com/e-CXPSZT7I8V76S2JSSVTHOPBE/lab



Azure
1. Create student account

2. Create a storage account

3. Create a container in the storage account
-- upload data to storage account
-- alternatively you can upload data to the container created for the cluster

4. Create HDInsight Cluster
-- make sure to select spark cluster
-- for more information, follow: https://docs.microsoft.com/de-de/azure/hdinsight/spark/apache-spark-jupyter-spark-sql
-- see separate file for configuration of cluster

5. Launch jupyter notebook



Databricks
1. Create student account

2. Create cluster

3. Upload aws credentials as .csv and create table from it

3. Start notebook on databricks notebook: https://community.cloud.databricks.com/?o=4954115768497647#notebook/1966248935447161/command/4310762031625674

4. install spark-nlp; see: (https://nlp.johnsnowlabs.com/docs/en/install#databricks, https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html#Getting%20Started.html, https://databricks.com/blog/2015/07/28/using-3rd-party-libraries-in-databricks-apache-spark-packages-and-maven-libraries.html)
-- add this to spark tab in cluster spark.kryoserializer.buffer.max 1000M
spark.serializer org.apache.spark.serializer.KryoSerializer
-- in Libraries tab inside cluster: Insatll New -> PyPI -> spark-nlp -> Install and Install New -> Maven -> Coordinates -> com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4 -> Install

